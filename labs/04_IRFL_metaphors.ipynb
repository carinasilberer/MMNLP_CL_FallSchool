{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV6Tko_O28Eh"
   },
   "source": [
    "# CL Fall School 2024 in Passau: Multimodal NLP\n",
    "Carina Silberer, University of Stuttgart\n",
    "\n",
    "*This notebook is based on the [evaluation notebook](https://colab.research.google.com/drive/1RfcUhBTHvREx5X7TMY5UAgMYX8NMKy7u?usp=sharing) of the authors of the work [IRFL: Image Recognition of Figurative Language](https://github.com/irfl-dataset/IRFL).*\n",
    "\n",
    "---\n",
    "\n",
    "# Lab 4: Metaphor Detection\n",
    "In this lab, we will use CLIP in a zero-shot setting to address the metaphor detection task as defined by [IRFL](https://irfl-dataset.github.io/). As rightfully pointed out, the task may be better considered a cross-modal inference task, but here we will study it the way it was defined by the authors of IRFL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required packages: Installation\n",
    "If one of the packages below are not yet installed on your computer, run the corresponding commands. \n",
    "For torchvision, see also https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJBs37Ts28b4"
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers --quiet\n",
    "!pip3 install -U datasets --quiet\n",
    "!pip3 install pip install tqdm --quiet\n",
    "!pip3 install fsspec==2023.6.0 --quiet\n",
    "!pip3 install torch torchvision --quiet\n",
    "!pip3 install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run just this to make sure you have compatible versions of pytorch and torchvision:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "import torchvision\n",
    "print(\"Torch vision version: \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import operator\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# We need additional packages for loading (and visualising) the images\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# from huggingface:\n",
    "from datasets import Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8skwO-Hx28oJ"
   },
   "source": [
    "## Warm-Up: Setting up the data\n",
    "### Loading the IRFL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QCCZ2LsPxIa"
   },
   "source": [
    "There is a dataset for each figure of speech (idiom, metaphor, simile) in IRFL. We will only work with metaphors, but feel free to also work on the other tasks (you need to adapt the script accordingly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0UZb2GC28yd"
   },
   "outputs": [],
   "source": [
    "# from huggingface:\n",
    "from datasets import Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loads the IRFL dataset from the huggingface hub\n",
    "IRFL_images = load_dataset(\"lampent/IRFL\", data_files='IRFL_images.zip')['train']\n",
    "\n",
    "# IRFL dataset of figurative phrase-image pairs (10k+ images)\n",
    "#IRFL_idioms_dataset = load_dataset(\"lampent/IRFL\", 'idioms-dataset')['dataset']\n",
    "#IRFL_similes_dataset = load_dataset(\"lampent/IRFL\", 'similes-dataset')['dataset']\n",
    "IRFL_metaphors_dataset = load_dataset(\"lampent/IRFL\", 'metaphors-dataset')['dataset']\n",
    "\n",
    "print('Successfully loaded IRFL dataset and metaphor task')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKQbSSxzQf1D"
   },
   "source": [
    "#### IRFL metaphor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFvt5zXGQf1J",
    "outputId": "6ff886b8-8c04-4902-a4c0-bb950a6c2f25"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(IRFL_metaphors_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50EeB8u8Pi3b"
   },
   "source": [
    "#### Retrieve and visualise an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "Z1WCwZH-3H3C",
    "outputId": "4d4ceeb5-c80b-4508-c57f-c5ce01d07f5f"
   },
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "\n",
    "def get_image(image_name, image_folder_path='data/IRFL/images/'):\n",
    "  return Image.open(os.path.join(image_folder_path, image_name.split(\".\")[0] + \".jpeg\")).convert(\"RGB\")\n",
    "\n",
    "image = get_image('105928442888727985035455816965889552425794851956670719249414602380285680963206')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVJUo6I6AtDK"
   },
   "source": [
    "## IRFL Multimodal Figurative Language Detection task\n",
    "The task is defined as follows: Given a metaphor and four candidate images, choose the image that conveys the metaphorical message. \n",
    "\n",
    "The approach we adopt is very simple: To find the \"correct\" image given a linguistic metaphor, the similarity between the metaphor and each of the four candidate images are measured, and the image with the highest similarity is chosen as the correct one (i.e., that conveys the meaning of the metaphor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa2gB-emN4O2"
   },
   "source": [
    "### Load IRFL Multimodal Metaphor Detection task\n",
    "We need to load the images (copy of the code above) as well as the task, i.e., the data of the task: the metaphors (*phrases*) and the image ids for each metaphor (4 candidates per metaphor, with 1 being the correct representation of the metaphor (*answer*) and three incorrect images (*distractors*)).\n",
    "\n",
    "We load the *test* split, since we want to evaluate a model on the task in a zero-shot setting (in contrast to training a model on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-wZPHejPPGC"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "IRFL_images = load_dataset(\"lampent/IRFL\", data_files='IRFL_images.zip')['train']\n",
    "\n",
    "def get_image(image_name, image_folder_path='data/IRFL/images/'):\n",
    "    return Image.open(os.path.join(image_folder_path, image_name.split(\".\")[0] + \".jpeg\")).convert(\"RGB\")\n",
    "\n",
    "# Detection task\n",
    "IRFL_metaphor_detection_task = load_dataset(\"lampent/IRFL\", 'metaphor-detection-task')[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 333 items in the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IRFL_metaphor_detection_task)\n",
    "IRFL_metaphor_detection_task.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xjDHmiHBf1S"
   },
   "source": [
    "### Approach: Image--Text Matching\n",
    "You can use any function that gives you a matching score between an image and a text. We will use CLIP, i.e., encode images and phrases with CLIP and then calculate the dot product between each phrase and its four candidate images (i.e., we obtain four matching scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2q_0lwfBdxp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "# we don't perform training, so we don't need \"gradient\" (gradient descent for backpropagation)\n",
    "total_parameters = sum(p.numel() for p in model.parameters())\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# If you want to work directly with representations, e.g. because you are using another approach, you can use this function\n",
    "def get_vectors_similarity(v1, v2):\n",
    "    similarity = v1.detach().cpu().numpy() @ v2.detach().cpu().numpy().T\n",
    "    return similarity\n",
    "\n",
    "def get_clip_similarity(phrase, images):\n",
    "    inputs = processor(text=phrase, images=images, return_tensors=\"pt\", padding=True)\n",
    "    # we use CLIP to calculate the dot product\n",
    "    outputs = model(**inputs)\n",
    "    return outputs\n",
    "\n",
    "# This function will return the matching probability of the phrase and its image.\n",
    "# If `definitions` are provided, it will concatenate the definitions of the phrase. Only idiom instances pass definitions.\n",
    "# If you wish to test your model replace CLIP with the desired model.\n",
    "def get_clip_phrase_image_similarity_score(phrase, img_name, definitions):\n",
    "    if definitions:\n",
    "        definition_prompt = '.'.join(definitions) + '.'\n",
    "        phrase += '.' + definition_prompt\n",
    "\n",
    "    image = get_image(img_name)\n",
    "    outputs = get_clip_similarity(phrase, image)\n",
    "    logits_per_image = outputs.logits_per_image.item()/100.0 # this is the image-text similarity score\n",
    "    return logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = '94721155644044834970605402726680935822854330271009500854369319195697461678173'\n",
    "phrase=\"burnt toast\"\n",
    "get_clip_phrase_image_similarity_score(phrase, img_name, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC1fDN7mFbeo"
   },
   "source": [
    "### Evaluation\n",
    "#### Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU8OtMgXBHjR"
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-learn --quiet\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def solve_IRFL_detection_task(task_instances):\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    for task_instance in tqdm(task_instances):\n",
    "        phrase, answer, distractors, candidates, definitions = preprocessing(task_instance)\n",
    "        prediction = solve_IRFL_detection_task_instance(phrase, candidates, definitions)\n",
    "\n",
    "        ground_truth.append(answer)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return ground_truth, predictions, int(accuracy_score(ground_truth, predictions) * 100)\n",
    "\n",
    "def solve_IRFL_detection_task_instance(phrase, candidates, definitions):\n",
    "    sim_for_image = {}\n",
    "    for img_name in candidates:\n",
    "        sim_for_image[img_name] = get_clip_phrase_image_similarity_score(phrase, img_name, definitions)\n",
    "\n",
    "    clip_prediction = Counter(sim_for_image).most_common()[0][0]\n",
    "    return clip_prediction\n",
    "\n",
    "def preprocessing(task_instance):\n",
    "    answer = json.loads(task_instance['answer'])\n",
    "    distractors = json.loads(task_instance['distractors'])\n",
    "    if task_instance.get('definition'):\n",
    "        definitions = json.loads(task_instance['definition'])\n",
    "    else:\n",
    "        definitions = None\n",
    "    candidates = answer+distractors\n",
    "    random.shuffle(candidates)\n",
    "    return task_instance['phrase'], answer[0], distractors, candidates, definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI56tmNEMkyF"
   },
   "source": [
    "#### Evaluate multimodal metaphor detection task\n",
    "We iterate over all instances, use CLIP to predict which image conveys the meaning of each corresponding metaphor, and compare the prediction with the true answers. \n",
    "To assess the model, we measure accuracy: The proportion of correct predictions of all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NBtxK_-M3b0",
    "outputId": "c4fbafca-fd69-453f-888e-8bd0f5e330ec"
   },
   "outputs": [],
   "source": [
    "ground_truth, predictions, accuracy = solve_IRFL_detection_task(IRFL_metaphor_detection_task)\n",
    "print(f\"model_accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "#### Plot examles with predictions and labels.\n",
    "True Positive: Correct model prediction marked in <font color='green'>green</font> color.\n",
    "</br>\n",
    "False Positive: Incorrect model prediction marked in <font color='red'>red</font> color.\n",
    "</br>\n",
    "Ground Truth: Incorrect model prediction marked in <font color='blue'>blue</font> color.\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RgsBhWHG9TO"
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib --quiet\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def plot_IRFL_detection_task_instance(phrase, candidates, answer, clip_prediction, score, definitons):\n",
    "  fig, axs = plt.subplots(1, 4, figsize=(40,20))\n",
    "  plot_text = f\"Phrase: {phrase}. Score: {score}.\"\n",
    "  if definitons:\n",
    "    plot_text = plot_text + f\" \\n Definitons: {definitons}\"\n",
    "  plt.suptitle(plot_text, fontsize=50)\n",
    "\n",
    "  plotted_img = 0\n",
    "  for ax_row in axs:\n",
    "    color = None\n",
    "    curr_img = get_image(candidates[plotted_img])\n",
    "    curr_candidate = candidates[plotted_img]\n",
    "    ax_row.imshow(curr_img)\n",
    "    height, width, _ = np.array(curr_img).shape\n",
    "    draw_rect = False\n",
    "    if curr_candidate == answer and curr_candidate == clip_prediction:\n",
    "      color = 'g'\n",
    "      draw_rect = True\n",
    "    elif curr_candidate == answer:\n",
    "      color = 'b'\n",
    "      draw_rect = True\n",
    "    elif curr_candidate == clip_prediction:\n",
    "      color = 'r'\n",
    "      draw_rect = True\n",
    "    plotted_img += 1\n",
    "    if color is not None:\n",
    "      rect = patches.Rectangle((0, 0), width, height, linewidth=20, edgecolor=color, facecolor='none')\n",
    "      ax_row.add_patch(rect)\n",
    "  ax = plt.gca()\n",
    "  ax.axes.xaxis.set_visible(False)\n",
    "  ax.axes.yaxis.set_visible(False)\n",
    "  ax.axes.xaxis.set_ticks([])\n",
    "  ax.axes.yaxis.set_ticks([])\n",
    "  plt.tight_layout()\n",
    "  return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_instance = IRFL_metaphor_detection_task[1]\n",
    "phrase, answer, distractors, candidates, definitions = preprocessing(task_instance)\n",
    "\n",
    "clip_prediction = solve_IRFL_detection_task_instance(phrase, candidates, None)\n",
    "fig, axs = plot_IRFL_detection_task_instance(phrase, candidates, answer, clip_prediction, 1 if answer == clip_prediction else 0, task_instance.get('definition',\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFPtQqAYZJ_b"
   },
   "source": [
    "## Exercise: Analysis\n",
    "Conduct a small analysis with the goal to get more insights into CLIP's predictions. Do this by performing zero-shot object classification: \n",
    "1. For each instance of one metaphor and four candidate images, have CLIP predict the top K labels for the four images. Then, compare the labels and the probability scores with CLIP's prediction on the metaphor task, i.e., which image did it predict as the one conveying the metaphor, and which class labels did it predict for the image? Are the class labels related to the metaphor?\n",
    "2. To do that, you will need a vocabulary of class labels. You create that by extracting the words of all phrases (metaphors).\n",
    "   Recall that for zero-shot classification with CLIP, CLIP expects a phrase as input. E.g., \"umbrella\" --> \"this is a photo of an umbrella .\"\n",
    "\n",
    "**Tasks**\n",
    "* Sample 20 images and analyse CLIP's predictions. Can you identify possible reasons for CLIP's (in)correct predictions?\n",
    "* Take also into account the instances themselves: Do they and the \"correct\" answer make sense to you?\n",
    "* Improve the set of class labels (vocabulary) by filtering out functions words, and/or applying lemmatisation. You need to write code for that. You may also devise a larger vocabulary.\n",
    "* If you, just for fun, also want to generate captions for the images, see the code at the bottom (GIT from the first lab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Data preparation and approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabulary of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(IRFL_metaphor_detection_task):\n",
    "    all_words = []\n",
    "    for task_instance in tqdm(IRFL_metaphor_detection_task):\n",
    "        phrase, _, _, _, _ = preprocessing(task_instance)\n",
    "        all_words.extend(phrase.split())\n",
    "    return list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRFL_metaphor_detection_task = load_dataset(\"lampent/IRFL\", 'metaphor-detection-task')[\"test\"]\n",
    "classes = load_labels(IRFL_metaphor_detection_task)\n",
    "prompts = [f\"this is not showing a {label} .\" for label in classes]\n",
    "prompts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The vocabulary could be improved, e.g., through lemmatisation or by discarding function words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach: Zero-shot classification with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for zero-shot classification\n",
    "def get_clip_scores(prompts, images):\n",
    "    probs = None\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "    return probs\n",
    "\n",
    "def clip_zero_chot_classification(texts, images, topKlabels=5):\n",
    "    probs = get_clip_scores(texts, images)\n",
    "    top_probs, top_labels = probs.cpu().topk(topKlabels, dim=-1)\n",
    "    return top_probs, top_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of the predictions (topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zero_shot_classification(phrase, images, top_probs, top_labels, classes):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plot_text = f\"Phrase: {phrase}.\"\n",
    "    plt.suptitle(plot_text, fontsize=12)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(4, 6, 3 * i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        plt.subplot(4, 6, 3 * i + 2)\n",
    "        y = np.arange(top_probs.shape[-1])\n",
    "        plt.grid()\n",
    "        plt.barh(y, top_probs[i])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        plt.yticks(y, [classes[index] for index in top_labels[i].numpy()])\n",
    "        plt.xlabel(\"probability\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions = True\n",
    "plot_zeroshot_classes = True\n",
    "\n",
    "IRFL_metaphor_detection_task = load_dataset(\"lampent/IRFL\", 'metaphor-detection-task')[\"test\"]\n",
    "# Sample an instance. You can also hard-code the parameter below to get a specific instance of the dataset\n",
    "instance_num = np.random.randint(IRFL_metaphor_detection_task.num_rows)\n",
    "\n",
    "# load an instance of the dataset\n",
    "task_instance = IRFL_metaphor_detection_task[instance_num]\n",
    "phrase, answer, distractors, candidates, definitions = preprocessing(task_instance)\n",
    "\n",
    "# if we do zero-shot classification, we create a vocabulary of classes \n",
    "# by extracting the words of all phrases (metaphors)\n",
    "if plot_zeroshot_classes:\n",
    "    classes = load_labels(IRFL_metaphor_detection_task)\n",
    "    prompts = [f\"this is a rendering of a {label} .\" for label in classes]\n",
    "    #prompts = [f\"this is a photo of a {label} .\" for label in classes]\n",
    "\n",
    "# do the metaphor detection task and plot the prediction\n",
    "if plot_predictions:\n",
    "    clip_prediction = solve_IRFL_detection_task_instance(phrase, candidates, None)\n",
    "    fig, axs = plot_IRFL_detection_task_instance(phrase, candidates, answer, clip_prediction, 1 if answer == clip_prediction else 0, task_instance.get('definition',\"\"))\n",
    "\n",
    "# do zero-shot classification and plot the top-K predicted classes\n",
    "print(\"Instance index: \", instance_num)\n",
    "if plot_zeroshot_classes:\n",
    "    images = [get_image(img_name) for img_name in candidates]\n",
    "    top_probs, top_labels = clip_zero_chot_classification(prompts, images)\n",
    "    plot_zero_shot_classification(phrase, images, top_probs, top_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Captioning the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "## Two different models, trained on textcaps and vatex, resp.\n",
    "#git_processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textcaps\", clean_up_tokenization_spaces=True)\n",
    "#git_model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textcaps\")\n",
    "git_processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\", clean_up_tokenization_spaces=True)\n",
    "git_model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "\n",
    "# run on the GPU if you have one\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# we don't perform training, so we don't need \"gradient\" (gradient descent for backpropagation)\n",
    "total_parameters = sum(p.numel() for p in git_model.parameters())\n",
    "for param in git_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [get_image(img_name) for img_name in candidates]\n",
    "\n",
    "for idx, img in enumerate(images):\n",
    "    inputs = git_processor(images=img, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(device)\n",
    "    generated_ids = git_model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    caption = git_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print(\"Generated caption:\", caption)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "MQSHzcgA285w",
    "JKQbSSxzQf1D",
    "6W78HB3zQgHr",
    "50EeB8u8Pi3b",
    "wa2gB-emN4O2",
    "1xjDHmiHBf1S",
    "GC1fDN7mFbeo",
    "dFPtQqAYZJ_b",
    "k4iIqfDJN4O5",
    "mKCL4O3NRjs5",
    "UjBn75XfRjs5"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01453c7626894a41a65c30dd5b27c340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09c2526c85974c9da444a066906847bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15f106c88d1f49c5b0528c34bd81d35f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "503281dbb8b44804a44275801895c8ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d2a2f2bca0a41b48578c72cc104d546",
      "placeholder": "​",
      "style": "IPY_MODEL_ead67388c7cb42e584c430ed4cc366d4",
      "value": " 200/200 [00:00&lt;00:00, 4280.38 examples/s]"
     }
    },
    "51b0639c4b974b2b92586e1f327e8420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5cfebc99fa084c99a5fe4921be17766f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09c2526c85974c9da444a066906847bf",
      "placeholder": "​",
      "style": "IPY_MODEL_01453c7626894a41a65c30dd5b27c340",
      "value": "Filter: 100%"
     }
    },
    "6df75ac9c30d454991e49c25396cde42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d2a2f2bca0a41b48578c72cc104d546": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0a9bd1d93534a3bbf5b43b436a5e1f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15f106c88d1f49c5b0528c34bd81d35f",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51b0639c4b974b2b92586e1f327e8420",
      "value": 200
     }
    },
    "ead67388c7cb42e584c430ed4cc366d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f612ecb27fdb4bf2965474de7addbd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5cfebc99fa084c99a5fe4921be17766f",
       "IPY_MODEL_e0a9bd1d93534a3bbf5b43b436a5e1f4",
       "IPY_MODEL_503281dbb8b44804a44275801895c8ae"
      ],
      "layout": "IPY_MODEL_6df75ac9c30d454991e49c25396cde42"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
