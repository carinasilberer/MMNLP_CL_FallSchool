{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b823a8a-8d32-4b59-9bd3-970ae45908da",
   "metadata": {},
   "source": [
    "# CL Fall School 2024 in Passau: Multimodal NLP\n",
    "Carina Silberer and Hsiu-Yu Yang, University of Stuttgart\n",
    "\n",
    "---\n",
    "\n",
    "# Lab 3: Word Similarity Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4e5a0-243d-4361-b342-2c4aab9b2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9acc5-6217-4dd8-8c46-46353130e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision -c pytorch-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a4724-1001-47e0-bd2e-91ee1795fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee653e4-2950-4930-b0a1-482368e70c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import pandas\n",
    "except ModuleNotFoundError:\n",
    "    #!conda update -n base -c defaults conda\n",
    "    !conda install --yes pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1cb9d8-227a-403d-915f-fcbdf766d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00712cda-e17b-4dcc-849b-d4fa75db3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550020c9-3d74-40d4-9936-43fe668e0ef5",
   "metadata": {},
   "source": [
    "# Exercise: Word Similarity Estimation\n",
    "Word similarity and relatedness datasets have long been used to intrinsically evaluate distributional representations of word meaning. The standard evaluation metric for such datasets is the [Spearman correlation coefficient (Spearman's $\\rho$)](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). \n",
    "It is computed between the human-elicited scores and your model's estimated scores.\n",
    "\n",
    "The goal of this exercise is to compare 3 classes of models, a `pure language model`, a `pure vision model` and a `vision-language model` on the word similarity task. \n",
    "\n",
    "### Dataset: SimLex-999\n",
    "We will use the word pairs and human similarity judgements of SimLex-999.\n",
    "Download the dataset either from the course's github space (under `data/`), or from the website (https://fh295.github.io/simlex.html), the filename is `SimLex-999/SimLex-999.txt`. Check also the description of the dataset in the `README`. The relevant data for this assignment are provided in the columns `word1`, `word2`, `POS`, `SimLex999` (scale 0-10), and `concQ` (derived from  concreteness ratings (scale 1-7) for the individual words of a pair). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b80f6-c37a-42b1-a9c0-9655549159d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = pandas.read_csv(\"data/SimLex-999/SimLex-999.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b1104-8c9b-45b0-8782-a50a06c90b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 10 entries in SimLex-999\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c7c616-7de8-4568-9540-8fbc65618f47",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "We load the models and prepare them and the vocabulary, and use Spearman's $\\rho$ to measure the correlation between the human-elicited similarity judgements and the model's estimated similarity scores. \n",
    "\n",
    "To cleanly disentangle the contribution of the respective modality, ensure the selected models have similar backbone/architecture. \n",
    "For example, ViLT's vision backbone is ViT. Ideally, we would also use ViLT's textual backbone, but since that was trained from scratch, we use the commonly used linguistic encoder BERT.\n",
    "\n",
    "* `Language model`: [BERT-base](https://huggingface.co/google-bert/bert-base-uncased) (**BERT**)\n",
    "* `Vision model`: [VIT](https://huggingface.co/docs/transformers/model_doc/vit#overview) (**ViT**)\n",
    "* `Vision-language model`: [ViLT](https://huggingface.co/docs/transformers/model_doc/vilt) (**ViLT**)\n",
    "\n",
    "\n",
    "##### Procedure:\n",
    "1. Step 1: (For vision-based models) Prepare visual input for words in the SimLex-999 dataset.\n",
    "2. Step 2: Load and prepare the models\n",
    "3. Step 3: Use the the models to extract the words and images' representation for calculating similarity scores\n",
    "4. Step 4: Calculating similarity scores\n",
    "5. Step 5: Use Spearman's $\\rho$ to measure the correlation between the human-elicited similarity judgements and the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157e1d2-3a7d-4348-9a87-deb40dc9b00e",
   "metadata": {},
   "source": [
    "### Step 1: Loading the images\n",
    "We need visual representations for the target concepts (words) in the form of images. \n",
    "In practice, one can search for the images online to match a word in the unimodal dataset.\n",
    "(*note: in distribution*)\n",
    "\n",
    "In this lab, let's try to use the most common image-caption dataset for vision-based model pretraining, Conceptual Captions ([CC3M](https://ai.google.com/research/ConceptualCaptions/download)), and retrieve images from there showing our target words as their visual representation.\n",
    "In the `data/CC3M` folder, you will find a file called `topic2image.json`, which gives a list of image ids for each topic (i.e., word). `imageID2url.json`, in turn, gives the corresponding urls.\n",
    "Each topic word has **at most 10 images** sorted by so called tf-idf values (the highest, the more associated it is to the word).\n",
    "\n",
    "**To save time, the images that are available for WordSim-999 have been already downloaded. You will find them in a google drive (see slack). Download them, put them under data/CC3M/ and unzip them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aecf17-6da4-43a7-9e7a-d5225b6bc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78594166-a7d9-414f-85d8-b584ea4305a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/CC3M/\"\n",
    "imgname = os.listdir(os.path.join(datadir, \"images/\"))[0]\n",
    "print(imgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5298bdc-ac5e-4105-ac2f-f9e5de9e1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the words for which we found images in CC3M\n",
    "with open(os.path.join(datadir, \"words_w_imgs.txt\")) as f:\n",
    "    wordsinCC3M = [w.strip() for w in f.readlines()]\n",
    "\n",
    "print(f'Num of words in CC3M: {len(wordsinCC3M)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301d4ae-c526-49f2-afda-a7cfe5a36b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the subset of words from SimLex999 that have images in CC3M for analysis\n",
    "sim_data = pandas.read_csv(os.path.join(\"data/SimLex-999\", \"SimLex-999.txt\"), sep=\"\\t\")\n",
    "\n",
    "# Check how many words are overlapped between SimLex999 and CC3M\n",
    "sim_words = set(sim_data['word1']).union(set(sim_data['word2']))\n",
    "overlapped_words = sim_words.intersection(set(wordsinCC3M))\n",
    "print(len(overlapped_words), 'overlapped words between SimLex999 and CC3M')\n",
    "print('Words:', list(overlapped_words)[:5])\n",
    "\n",
    "# For similarity analysis, we can only keep the rows for whose words we have the corresponding images\n",
    "new_sim_data = sim_data[sim_data['word1'].isin(overlapped_words) & sim_data['word2'].isin(overlapped_words)]\n",
    "print('Num of word pairs that have images for analysis:', len(new_sim_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceab7b8-796b-4ed4-aa12-54e5d3c1287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e8f7d-6e4a-4dc2-aa7c-0feddecaab2d",
   "metadata": {},
   "source": [
    "We need to load images to represent visually a concept which we do with the function `get_images`. <br/>\n",
    "*Remark to the function `get_images`:* If an identified image that shows corresponding objects is not locally in your data folder yet, we'll download it using its url and save it locally. Otherwise we directly load it from the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374b02f-21a5-490b-9164-ebd68be2ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a helper function to access the image url easier\n",
    "import copy\n",
    "def get_images(word, num=3, download=False):\n",
    "    images = []\n",
    "    for i in range(num):\n",
    "        filepath = os.path.join(datadir, \"images/\", word+str(i)+\".jpg\")\n",
    "        if os.path.exists(filepath):\n",
    "            #image = Image.open(filepath)\n",
    "            image = copy.deepcopy(Image.open(filepath)).convert('RGB')\n",
    "            images.append(image)\n",
    "        elif download==True:\n",
    "            with open(os.path.join(datadir, \"topic2image.json\")) as f:\n",
    "                topic2image = json.load(f)\n",
    "            with open(os.path.join(datadir, \"imageID2url.json\")) as f:\n",
    "                imageID2url = json.load(f)\n",
    "            image_IDs = [item[1] for item in topic2image.get(word)]\n",
    "            loaded_imgs = 0\n",
    "            for j in range(len(image_IDs)):\n",
    "                if i+loaded_imgs >= num-1:\n",
    "                    break\n",
    "                url = imageID2url.get(image_IDs[j])\n",
    "                #print(url)\n",
    "                try:\n",
    "                    img = Image.open(requests.get(url, stream=True).raw)\n",
    "                    img.save(os.path.join(datadir, \"images/\", word+str(i+j)+\".jpg\"))\n",
    "                    images.append(img.convert('RGB'))\n",
    "                    loaded_imgs += 1\n",
    "                except:\n",
    "                    print(f'Failed to load image {url}')\n",
    "    #print(f'Loaded {len(images)} images for \"{word}\"')\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de140cb7-67c9-4f16-ae80-d1b3ca2e7bb4",
   "metadata": {},
   "source": [
    "### Step 2: Loading the models\n",
    "* We will use ViT to encode images\n",
    "* To encode text, we will use ?\n",
    "* Finally, to encode image + text, i.e., to get a visual--linguistic representation, we will use ViLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9c449-ae22-4be0-8000-9b15e489d836",
   "metadata": {},
   "source": [
    "#### Load the visual encoder model (ViT)\n",
    "We will use [ViT-32](https://huggingface.co/google/vit-base-patch32-224-in21k), the visual transformer model we discussed in class. It was  trained on ImageNet, which is a huge collection of images labeled with the objects they show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27d052-1c24-4ab0-b071-e6bbeb1f8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIT \n",
    "# We need a processor to read in images in pixel values\n",
    "from transformers import ViTImageProcessor\n",
    "from transformers import ViTModel\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch32-224-in21k')\n",
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch32-224-in21k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11137671-8d33-4298-a3ad-1ab6e12abdcd",
   "metadata": {},
   "source": [
    "#### Load the text encoder model (BERT)\n",
    "We will use [BERT-base](https://huggingface.co/google-bert/bert-base-uncased) to encode text. It is not a state-of-the-art representation model anymore, but has been used to initialise the textual encoder for many VL models (including CLIP). This means, BERT was used as a starting point in other models to embed (represent) the textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87c3f5-a1e8-4e9b-842d-bb0b0a32708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=True)\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642dd58-d394-445d-8127-e0f493cc29cc",
   "metadata": {},
   "source": [
    "#### Load the visual--linguistiv model (ViLT)\n",
    "To represent jointly image+text, we will encode the two modalities using the VL model [ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eba36e-7ed0-46e1-b65c-7c024d5ea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor\n",
    "from transformers import ViltModel\n",
    "\n",
    "mm_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\", clean_up_tokenization_spaces=True)\n",
    "mm_model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a9088-8cee-4d80-9ab1-329f1eba9666",
   "metadata": {},
   "source": [
    "### Step 3a: Loading and preparing the data\n",
    "We need to load the images and the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5824fd-a166-432c-b96d-33b7e7729503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image and text\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"two cats with two remote controls\"\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68f0a1-43c7-4e2b-84f6-f97aa16dbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images(\"cat\")\n",
    "text = \"cat\"\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb41257-b425-4235-ad42-ea7159afcaba",
   "metadata": {},
   "source": [
    "### Step 3b: Encoding the images and words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed0527-91b7-40ed-b2d7-97eadaa0f90f",
   "metadata": {},
   "source": [
    "#### Encode the image --> feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684f666-a795-4f60-b1ff-adc266031a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pixel values of the image to prepare the model input\n",
    "#inputs = image_processor(images=word1_images, return_tensors=\"pt\").to(device)\n",
    "inputs = image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = vit_model(**inputs)\n",
    "    # We take the [CLS] token representation of the last layer as the representation of the whole image\n",
    "    pooler_output = outputs.pooler_output\n",
    "    print(pooler_output.shape) # batch size is 2 because there are two images for word1\n",
    "    \n",
    "    # Get the average of the image embeddings as the representation for a word (if there were more than one image for the text)\n",
    "    image_embedding = pooler_output.mean(dim=0).unsqueeze(0)\n",
    "    print(f'Averaged image representation for \"{text}\":', image_embedding.shape) # batch size: 1, hidden size: 768 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee520b1-ae1f-4661-95f6-4be562c5be78",
   "metadata": {},
   "source": [
    "#### Encode the text --> feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760df85-df49-4907-af86-ea712557d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = text_model(**encoded_input)\n",
    "    pooler_output = outputs.pooler_output\n",
    "    print(f'Representation size for input \"{text}\":', pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5dbd8-880a-430c-bb60-1c535a734850",
   "metadata": {},
   "source": [
    "#### Encode the image + word --> multimodal feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef885bb2-28f2-472a-a0ee-d2db76272e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A picture of a {}.\"\n",
    "words4vilt = len(images)*[prompt.format(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72952b-e3dc-4cc8-aa0a-8e0b80306559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViLT multimodal embeddings\n",
    "inputs = mm_processor(images, words4vilt, return_tensors=\"pt\")\n",
    "outputs = mm_model(**inputs)\n",
    "#last_hidden_states = outputs.last_hidden_state\n",
    "mm_pooler_output = outputs.pooler_output\n",
    "print(mm_pooler_output.shape)\n",
    "\n",
    "# Get the average of the image-text embeddings as the representation for a text+image (if there were more than one image for the text)\n",
    "mm_embedding = pooler_output.mean(dim=0).unsqueeze(0)\n",
    "print(f'Averaged multimodal representation for \"{text}\" and its images:', mm_embedding.shape) # batch size: 1, hidden size: 768 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099be140-b2c9-470e-a8b7-640feafbc3d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe46261-aadc-4fb1-a31f-228fbee50541",
   "metadata": {},
   "source": [
    "**Let's have a helper function that gives us the embedding of the respective modalities:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d8030-02b5-4360-935b-65f790b91d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you can adapt the following code on a bigger batch size, but be aware of the memory usage\n",
    "def get_representation(model, input_processer, device, inputs, modality=\"Text\"):\n",
    "    returned_embedding = None\n",
    "    if modality == \"Text\":\n",
    "        input2model = input_processer([inputs['word']], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input2model)\n",
    "            last_hidden_states = outputs.last_hidden_state[:, :-1, :] # disregard the last token </s>\n",
    "            # we average the hidden states of the tokens (if > 1) to get the representation of the word\n",
    "            word_embedding = last_hidden_states.mean(dim=1)\n",
    "            returned_embedding = word_embedding\n",
    "    elif modality == \"Image\":\n",
    "        in_images = inputs['images']\n",
    "        #in_images = [img.convert('RGB') for img in inputs['images']]\n",
    "        input2model = input_processer(images=in_images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input2model)\n",
    "            pooler_output = outputs.pooler_output\n",
    "            # get the average representation of the images\n",
    "            image_embedding = pooler_output.mean(dim=0).unsqueeze(0)\n",
    "            returned_embedding = image_embedding\n",
    "    elif modality == \"Image+Text\":\n",
    "        prompt = \"A picture of a {}.\"\n",
    "        text = inputs[\"word\"]\n",
    "        in_images = inputs['images']\n",
    "        #in_images = [img.convert('RGB') for img in inputs['images']]\n",
    "        words4vilt = len(in_images)*[prompt.format(text)] # we have three images, so we input the word for each image, i.e., have a list that contains the target word three times\n",
    "        input2model = input_processer(in_images, words4vilt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**input2model)\n",
    "        #last_hidden_states = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "\n",
    "        # Get the average of the image-text embeddings as the representation for a text+image (if there were more than one image for the text)\n",
    "        returned_embedding = pooler_output.mean(dim=0).unsqueeze(0)\n",
    "        \n",
    "    return returned_embedding.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4cff3-fa87-47c8-9501-b8b247d355b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "images = get_images(\"fast\")\n",
    "inputs = {\"word\": \"fast\", \"images\": images}\n",
    "\n",
    "print('BERT:', get_representation(text_model, tokenizer, device, inputs, modality=\"Text\").shape)\n",
    "print('ViT Base:', get_representation(vit_model, image_processor, device, inputs, modality=\"Image\").shape)\n",
    "print('ViLT:', get_representation(mm_model, mm_processor, device, inputs, modality=\"Image+Text\").shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9885bf-d7dd-41e4-b87d-7c15f5307d94",
   "metadata": {},
   "source": [
    "### Step 4: Compute the Similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059fea1-627c-474b-baa5-e0b184c425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the similarity predictions with the model for a word pair\n",
    "# Take a random row\n",
    "row=4\n",
    "word1, word2 = new_sim_data.iloc[row]['word1'], new_sim_data.iloc[row]['word2']\n",
    "print(word1, word2)\n",
    "\n",
    "# Get the images for the words\n",
    "num_of_images = 3\n",
    "word1_images = get_images(word1, num=num_of_images)\n",
    "word2_images = get_images(word2, num=num_of_images)\n",
    "if len(word1_images) < 1 or len(word2_images) < 1:\n",
    "    print(\"Not enough images.\")\n",
    "\n",
    "# Get the representations for the words\n",
    "# Alternatively, you can extract the embeddings beforehand for all words and save it in a pickle file for computation efficiency\n",
    "\n",
    "# Visual model\n",
    "inputs = {\"word\": word1, \"images\": word1_images}\n",
    "word1_embedding = get_representation(vit_model, image_processor, device, inputs, modality=\"Image\") # 0.12\n",
    "inputs = {\"word\": word2, \"images\": word2_images}\n",
    "word2_embedding = get_representation(vit_model, image_processor, device, inputs, modality=\"Image\")\n",
    "\n",
    "# Compute the similarity score\n",
    "similarity_score = cosine_similarity(word1_embedding, word2_embedding)\n",
    "print(f'Visual model: Similarity score between \"{word1}\" and \"{word2}\":', similarity_score[0][0])\n",
    "\n",
    "# Textual model\n",
    "inputs = {\"word\": word1, \"images\": word1_images}\n",
    "word1_embedding = get_representation(text_model, tokenizer, device, inputs, modality=\"Text\") # 0.12\n",
    "inputs = {\"word\": word2, \"images\": word2_images}\n",
    "word2_embedding = get_representation(text_model, tokenizer, device, inputs, modality=\"Text\")\n",
    "\n",
    "# Compute the similarity score\n",
    "similarity_score = cosine_similarity(word1_embedding, word2_embedding)\n",
    "print(f'Textual model: Similarity score between \"{word1}\" and \"{word2}\":', similarity_score[0][0])\n",
    "\n",
    "# Multimodal model\n",
    "inputs = {\"word\": word1, \"images\": word1_images}\n",
    "word1_embedding = get_representation(mm_model, mm_processor, device, inputs, modality=\"Image+Text\") # 0.59\n",
    "inputs = {\"word\": word2, \"images\": word2_images}\n",
    "word2_embedding = get_representation(mm_model, mm_processor, device, inputs, modality=\"Image+Text\")\n",
    "\n",
    "# Compute the similarity score\n",
    "similarity_score = cosine_similarity(word1_embedding, word2_embedding)\n",
    "print(f'Multimodal model: Similarity score between \"{word1}\" and \"{word2}\":', similarity_score[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f921e1-127c-43ed-818b-aee24c92c4c5",
   "metadata": {},
   "source": [
    "## Putting it all together: Computing the similarities for all word pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdf687-ebe3-403b-905c-2815737f9279",
   "metadata": {},
   "source": [
    "Below are some helper functions which do what was shown above, just in condensed form and for all word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9b854-4241-442d-bd05-2947a0569efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(word, num_of_images=3):\n",
    "    \"\"\" For a given word, extract <num_of_images> images that show (presumably) the object denoted by the word.\n",
    "    \"\"\"\n",
    "    word_images = get_images(word, num=num_of_images)\n",
    "    model_inputs = {\"word\": word, \"images\": word_images}\n",
    "    return model_inputs\n",
    "\n",
    "def calculate_similiarity_score(word1, word2, type, embeddings):\n",
    "    word1_embedding = embeddings.get(word1)[type]\n",
    "    word2_embedding = embeddings.get(word2)[type]\n",
    "    similarity_score = cosine_similarity(word1_embedding, word2_embedding)\n",
    "    return similarity_score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f148ba6-9734-4ae0-87af-823ffa8180b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model inputs from words in the dataset\n",
    "new_sim_data.loc[:,'word1_model_inputs'] = new_sim_data['word1'].apply(get_model_inputs)\n",
    "new_sim_data.loc[:,'word2_model_inputs'] = new_sim_data['word2'].apply(get_model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745236e6-b2ea-4f9c-afaf-5911f9879bb7",
   "metadata": {},
   "source": [
    "It's more efficient to extract the representations for all the words once and save them in a pickle file. The code below does that, and saves the embeddings under `my_lab_embeddings.pkl`. However, you can also directly load the embeddings in the code below (else-block; the file is called `03_embeddings.pkl`, see the github repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a789629-a6a1-4a0a-ba9b-ecc55c7a39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# set to True if you want to extract the embeddings yourself\n",
    "extract_embeddings = False\n",
    "\n",
    "# ------Streamline the pipeline------\n",
    "# Prepare model inputs from words in the dataset\n",
    "#new_sim_data['word1_model_inputs'] = new_sim_data['word1'].apply(get_model_inputs)\n",
    "#new_sim_data['word2_model_inputs'] = new_sim_data['word2'].apply(get_model_inputs)\n",
    "\n",
    "if extract_embeddings:\n",
    "    # Extract the embeddings and save them\n",
    "    all_reprs = {}\n",
    "    for i in range(len(new_sim_data)):\n",
    "        for col in ['word1_model_inputs', 'word2_model_inputs']:\n",
    "            model_inputs = new_sim_data.iloc[i][col]\n",
    "            word = model_inputs['word']\n",
    "            text_based_representation = get_representation(text_model, tokenizer, device, model_inputs, modality=\"Text\")\n",
    "            image_based_representation = get_representation(vit_model, image_processor, device, model_inputs, modality=\"Image\")\n",
    "            image_text_based_representation = get_representation(mm_model, mm_processor, device, model_inputs, modality=\"Image+Text\")\n",
    "            all_reprs[word] = {\"text_based\": text_based_representation, \n",
    "                                \"image_based\": image_based_representation, \n",
    "                                \"image_text_based\": image_text_based_representation}\n",
    "    \n",
    "    save_to_file=True\n",
    "    if save_to_file:\n",
    "        # save the embedding using pickle\n",
    "        with open(f'my_lab_embeddings.pkl', 'wb') as f:\n",
    "            print('Saving the embeddings to embeddings.pkl')\n",
    "            pickle.dump(all_reprs, f)\n",
    "else:\n",
    "    # Read the embeddings from the pickle file\n",
    "    with open(f'03_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eea3de-ed15-4a29-b3d4-ec8b50268133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the similarity scores for all word pairs\n",
    "new_sim_data.loc[:,'text_based_similarity_score'] = new_sim_data.apply(lambda x: calculate_similiarity_score(x['word1'], x['word2'], 'text_based', embeddings), axis=1)\n",
    "new_sim_data.loc[:,'image_based_similarity_score'] = new_sim_data.apply(lambda x: calculate_similiarity_score(x['word1'], x['word2'], 'image_based', embeddings), axis=1)\n",
    "new_sim_data.loc[:,'image_text_based_similarity_score'] = new_sim_data.apply(lambda x: calculate_similiarity_score(x['word1'], x['word2'], 'image_text_based', embeddings), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ca390-e2c0-4fdb-ad84-61b1145df912",
   "metadata": {},
   "source": [
    "### Step 5: Measuring Spearman's $\\rho$\n",
    "Now where we obtained cosine similarities for all word pairs computed with the three different models (the representations extracted with them from the text and the images), we can compute in how far the scores (i.e., the models) can account for human judgements on word similarity. \n",
    "We do this by computing Spearman's rank correlation between the ranking of the scores of a model with the ranking of the similarity judgements elicited from humans (average). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb220380-3913-4bc3-8387-c9162a7285fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688a105-4096-4e5f-82d9-a9a0ea5c2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity predictions with the model for the test word pairs,\n",
    "# and compare them against the human ratings, using Spearman's rho:\n",
    "rho_text = spearmanr(new_sim_data[\"SimLex999\"], new_sim_data[\"text_based_similarity_score\"])\n",
    "rho_vis = spearmanr(new_sim_data[\"SimLex999\"], new_sim_data[\"image_based_similarity_score\"])\n",
    "rho_vt = spearmanr(new_sim_data[\"SimLex999\"], new_sim_data[\"image_text_based_similarity_score\"])\n",
    "\n",
    "print(\"Spearman's rho for the textual model: {} (p-value: {}\".format(rho_text.correlation, rho_text.pvalue))\n",
    "print(\"Spearman's rho for the visual model: {} (p-value: {}\".format(rho_vis.correlation, rho_vis.pvalue))\n",
    "print(\"Spearman's rho for the vl model: {} (p-value: {}\".format(rho_vt.correlation, rho_vt.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08032d57-acea-4755-afc2-c6ad8cb4b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the analyses below\n",
    "def subset_spearman(subset, pred_column='text_based_similarity_score'):\n",
    "    return spearmanr(subset[\"SimLex999\"], subset[pred_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9a46c-1f37-408f-82c2-f4dcd73eba30",
   "metadata": {},
   "source": [
    "#### Random Baseline\n",
    "It is useful to compare a model against a baseline. We'll use a random baseline, that assigns a random similarity value (between 0 and 1) to each test word pair. \n",
    "Evaluate it on the word similarity task measuring Spearman's $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a507e-a7a7-480b-97b3-9d8f297d3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline\n",
    "# conveniently, np.random.rand produces floats between 0.0 and 1.0\n",
    "# the same range as the word similarity\n",
    "import numpy\n",
    "rand_vals = numpy.random.rand(len(new_sim_data))\n",
    "new_sim_data.loc[:,\"random\"] = rand_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df719b-6b7e-4dfc-b258-41f0650622af",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde67cce-9026-469d-95fb-d09d0a6de87c",
   "metadata": {},
   "source": [
    "## Exercises: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae75058-f6ce-460f-aaf2-85716d6ac782",
   "metadata": {},
   "source": [
    "### Evaluation: Error Analysis\n",
    "For error analysis, we can look at the words with the largest delta between the gold score and the predicted similarity score of the glove model. Analoguously to our evaluation metric (Spearman's $\\rho$), we base our comparison on ranks. We normalise both rankings to make their levels comparable. <br/>\n",
    "\n",
    "(Link to a pandas' `Series` method we will use for that:\n",
    "https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.rank.html#pandas-series-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c6616-0e3d-4524-8a7f-5bc9e099ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalised_ranking(sim_list):\n",
    "    ranks = pandas.Series(sim_list).rank(method='dense')  #'average')\n",
    "    return ranks / ranks.sum()\n",
    "\n",
    "def error_analysis(predictions, reference_scores, word_pairs):\n",
    "    predictions_rank = _normalised_ranking(predictions)\n",
    "    refscore_rank = _normalised_ranking(reference_scores)\n",
    "    error =  abs(predictions_rank - refscore_rank)\n",
    "    return sorted(zip(word_pairs, error, predictions, reference_scores), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc3ec6-f5c1-49bf-9b33-7e534fa77c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ten worst and ten best predictions, respectively\n",
    "def analyze(subset, pred_column='text_based_similarity_score', target_column=\"SimLex999\"):\n",
    "    rank_diffs = error_analysis(\n",
    "        subset[pred_column],\n",
    "        subset[target_column],\n",
    "        [f'{first}-{second}' for first, second in subset[[\"word1\", \"word2\"]].to_numpy()]\n",
    "    )\n",
    "    print(\"Ten worst predictions (ranks differ the most): \", rank_diffs[:10])\n",
    "    print(\"\\nTen best predictions (ranks differ the least): \", rank_diffs[-10:])\n",
    "\n",
    "analyze(new_sim_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730fb7ec-880e-4f35-983c-cd3f8b4e05e6",
   "metadata": {},
   "source": [
    "### Quantitative Results\n",
    "Compare the correlation coefficients you obtained for the models. Make sure that the models have the same underlying vocabulary which should comprise only those word pairs for which each model has representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1615bc-f96c-46b0-96b7-f201a1771726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_spearman(new_sim_data, pred_column='text_based_similarity_score'))\n",
    "print(analyze(new_sim_data, pred_column='text_based_similarity_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e1a71-c834-41f8-8f2b-841987177983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe60d0-02d0-443a-bfc7-259d3c8a50c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925e99b-4c18-4e01-90c1-00c9032f0cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e86d6eed-67a6-4132-8d7e-7e94bd2aa010",
   "metadata": {},
   "source": [
    "### Qualitative Analysis (Error Analysis)\n",
    "1. Evaluate the models on a subset of word pairs\n",
    "  1. Separately for each word category (column `POS`), i.e., nouns, verbs, adjectives\n",
    "  2. Separately for abstract and concrete word pairs (column `concQ`). Do this for each of the 4 quartiles (from abstract to concrete).\n",
    "2. Inspect the worst and best predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67f997-55e1-4b11-8ca3-54c00b1dbf68",
   "metadata": {},
   "source": [
    "***Qualitative Analysis 1A:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0ae04-2129-4320-8694-ccbac79045cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjectives only:\n",
    "adjective_subset = new_sim_data[new_sim_data[\"POS\"] == \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62d75c-b621-4bac-9fc3-99ab10c228b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's rho between text model and humans for adjectives only:\n",
    "subset_spearman(adjective_subset, pred_column='text_based_similarity_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a09a2-8f7e-426d-9eb6-87a78489c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gloVe:\n",
    "analyze(adjective_subset, pred_column='text_based_similarity_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edcfa5-75e6-49b9-aeda-c3a428d78e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93bf22-dec5-4451-84db-6fb6222e9d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b86286-b312-4a25-9912-81ff5a04f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs only:\n",
    "verb_subset = new_sim_data[new_sim_data[\"POS\"] == \"V\"]\n",
    "\n",
    "# TODO: Your code here for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d4cd3-ce8c-439b-9ab7-77544259ca74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e89b70-a177-4452-93d8-e80a2e3844a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nouns only:\n",
    "noun_subset = new_sim_data[new_sim_data[\"POS\"] == \"N\"]\n",
    "subset_spearman(noun_subset)\n",
    "\n",
    "# TODO: Your code here for nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead08350-bfa8-401c-8d7d-b39bc323ec4a",
   "metadata": {},
   "source": [
    "***Qualitative Analysis 1B:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f0325-1b5c-44f3-918c-ee8ede156131",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quartile = new_sim_data[new_sim_data[\"concQ\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec7f73-f494-4980-aeff-24f50e88d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_spearman(first_quartile, pred_column='text_based_similarity_score'))\n",
    "print(analyze(first_quartile, pred_column='text_based_similarity_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab007a1-b84f-4194-9180-9c0d592dbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here for the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25a1c6-2546-4074-bc15-6a5671ca2518",
   "metadata": {},
   "source": [
    "**Task: Evaluate the models also on the second, third and fourth quartile.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2ae4f-a403-4010-bab9-932703da6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
