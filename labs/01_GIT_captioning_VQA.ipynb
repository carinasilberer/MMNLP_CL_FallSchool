{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497df7c0-9dab-4431-bf1f-bf90cbddf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f1d39-4100-416e-bbf0-ae17cd21f61b",
   "metadata": {},
   "source": [
    "# Lab 1: Classic language and vision tasks\n",
    "*This notebook is based on Niels Rogge's tutorial:*\n",
    "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/\n",
    "\n",
    "If you have a GPU, set ``runtime'' to GPU. With CPU, the process will be very slow.\n",
    "\n",
    "## GIT (GenerativeImage2Text)\n",
    "GIT is a transformer decoder that is based on CLIP (https://openai.com/index/clip/). We will learn more about CLIP and GIT in the next class. GIT gets an image (or several images to handle videos) and text as input, and generates text as the output. The model uses CLIP to encode the image as patch tokens, and the text as text tokens. Conditioned on both image and text tokens, the model predicts iteratively the next text tokens, given the image tokens and the previous(ly generated) text tokens.\n",
    "\n",
    "<img src=\"./git.jpeg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8efc1-fde1-4ea1-bef8-fdb3e3c04474",
   "metadata": {},
   "source": [
    "### Setup\n",
    "*Install transformers if you don't have them yet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be757e48-3e34-462a-97bf-72633dec584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bb894-b190-44de-8923-54481c32b5e2",
   "metadata": {},
   "source": [
    "*If torch is not installed yet, check how to install it on your device: (https://pytorch.org/get-started/locally)*\n",
    "For example, you may use <br/>\n",
    "`!pip3 install torch torchvision torchaudio` <br/>\n",
    "or <br/>\n",
    "`!conda install pytorch torchvision -c pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92593651-f4be-418f-898c-a693f60d3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2bee1-ef30-4761-8f10-d2b5f29b62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # to load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c5fe3-ccef-43ef-bfdc-441f14d7acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # python built-in package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48828e-1e31-4b93-a966-06d3c4a0003e",
   "metadata": {},
   "source": [
    "### Warm-up\n",
    "#### Loading an image\n",
    "Let's load an image and display it. We'll use images from the [IRFL dataset](https://irfl-dataset.github.io/), which is a collection of images used to study figurative language (we'll hear more about it later in the course). \n",
    "\n",
    "**To download the IRFL dataset, you can clone it into the lab folder. Create a `data` folder (for the data used in the labs), go to the data folder and type the following (the assumption is that you have git):**\n",
    "Make sure you have git-lfs installed (to handle large files, e.g., images), if not, see https://git-lfs.com (e.g., `brew install git-lfs` for mac)\n",
    "Then, in the terminal, type \n",
    "`git lfs install` and \n",
    "`git clone https://huggingface.co/datasets/lampent/IRFL`\n",
    "**You then only need to unzip the images and you should be good to go.**\n",
    "\n",
    "Assuming the images are stored at `data/IRFL/images`, we'll load the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d6c24-9851-4c94-b2dd-7ee39e179ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdir = \"data/IRFL/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed90e4-8a11-4b6f-be7c-5ae23af4a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgname = os.listdir(imgdir)[0] # list all the files in imgdir, get the first file\n",
    "print(imgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3ee2-116f-4277-9d71-6201651260a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(imgdir, imgname)\n",
    "image = Image.open(filepath).convert(\"RGB\") # convert the image to the red-green-blue colour scheme\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac695fa-bf0a-412f-8784-87379b33c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another image at index 17\n",
    "imgname = os.listdir(imgdir)[17]\n",
    "filepath = os.path.join(imgdir, imgname)\n",
    "image = Image.open(filepath).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37192167-2172-4d3b-870b-a200bc6e94dc",
   "metadata": {},
   "source": [
    "The image has 3 colour channels -- red, green and blue. We can also just display one component, e.g., green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078806ac-4d51-476f-8ffc-0c57c695632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image.getchannel('G'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a67e42-0a5c-48de-ba69-792c9eb14f6c",
   "metadata": {},
   "source": [
    "#### Preparing the image for GIT\n",
    "We use the `GitProcessor`, which includes an image processor (for the visual modality) and a tokeniser (for the linguistic modality). Feeding GIT an image will make it use the image processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d69b39-633d-4741-b135-5293fd3bb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# the Auto API automatically loads a GitProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textcaps\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075efc10-e1ba-4fb9-8440-9479496880eb",
   "metadata": {},
   "source": [
    "*The size of the loaded image (in a tensor) is 3x224x224 -- 3 colour channels (red, green, blue), and 224x224 pixels per colour channel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635f518-5a44-4dd1-8603-695d3819f7cf",
   "metadata": {},
   "source": [
    "#### Loading a GIT model\n",
    "We will load the GIT based-sized model from the [huggingface hub](https://huggingface.co/docs/hub/index), which was fine-tuned on the image captioning [TextCaps dataset](https://huggingface.co/datasets/lmms-lab/TextCaps). Loading it may take a while ...\n",
    "\n",
    "**Remark:** To see all the GIT models that are available on the hub, see here: https://huggingface.co/models?search=microsoft/git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22301088-a194-4253-a75d-d730c90857e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model with a causal language modeling \"head\", such that we can generate text with the model\n",
    "from transformers import AutoModelForCausalLM \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textcaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2dbd0-f79b-4c57-9481-67d0dff9ba5e",
   "metadata": {},
   "source": [
    "#### Generate a caption\n",
    "We call the `generate` method to generate a caption for the image. \n",
    "By default, *greedy decoding* is used, which, in order to generate a token *t*, chooses the token with the highest probability. Token *t+1* is then generated again by choosing the token with the highest probability given the image an the tokens *1, ..., t*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cbce9-2267-43e3-85bb-5d0fc563537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on the GPU if you have one\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=20)\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7da52b-9d69-4ca6-83fd-35ca9f54f95b",
   "metadata": {},
   "source": [
    "#### Functionalities\n",
    "Let's write a function to load an image to be input to GIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5961a-6ff5-4749-ba93-440ec116195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def load_image(imgdir=\"data/IRFL/images/\", imgname=None):\n",
    "    \"\"\" Loads an image and returns it in RGB format.\n",
    "        If no image directory is specified, images from IRFL will be used. \n",
    "        If imgname is None, a random image will be sampled and returned.\n",
    "    \"\"\"\n",
    "    if imgname == None:\n",
    "        imgname = random.sample(os.listdir(imgdir), 1)[0]\n",
    "    image = Image.open(os.path.join(imgdir, imgname)).convert(\"RGB\")\n",
    "    return image, imgname\n",
    "\n",
    "image, _ = load_image()\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3738d84-7a2f-4824-9771-681728b5c841",
   "metadata": {},
   "source": [
    "As an alternative, you can also used images from the web: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c16f3-3de5-460b-b2df-18b9ba0a4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_image(url, save_as):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_as, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/4/47/Jackfruit.jpeg'\n",
    "save_as = 'data/jackfruit.jpg'\n",
    "\n",
    "download_image(image_url, save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577d81c-e403-4c49-a279-7cd6c10d68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,_ = load_image(\"data\", \"jackfruit.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e89e6-9a07-477b-9ed1-fdac797e50a2",
   "metadata": {},
   "source": [
    "### Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1e3f2-2990-4401-942a-e4ec719cf3c8",
   "metadata": {},
   "source": [
    "The code below is the same as above, just condensed in one block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398125b4-3307-42e8-82f0-7244b10311fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textcaps\", clean_up_tokenization_spaces=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textcaps\")\n",
    "\n",
    "# run on the GPU if you have one\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8710987-33f4-4e7c-b18b-9903bbeb1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, imgname = load_image()\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "print(\"Image name: \", imgname)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab86d1-fc0a-48cf-a575-305899fdcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, imgname = load_image(\"data\", \"jackfruit.jpg\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "print(\"Image name: \", imgname)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7628e-e4ae-4703-87f2-7b485eaa4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 20\n",
    "img_names = []\n",
    "for idx in range(num_images):\n",
    "    img, imgname = load_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860eb73-7b67-4da7-85e1-fd81679705c8",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "* Using the code above, sample 20 images and generate a caption for each of them with GIT.\n",
    "Inspect the generated captions and evaluate them according to the following criteria:\n",
    "   1. Faithfulness/Fidelity: Is the caption *consistent* with the content of the image? Is the sentence truthfully related to what is shown in the image? Does it have *hallucinations* or *non-factual* information?\n",
    "   2. Adequacy: How much image gist does the caption contain? (That is, how exhaustively does it describe the image content?)\n",
    "   4. Fluency: Is the sentence fluent, grammatical and coherent (irrespective of the image)?\n",
    "   5. Informativeness: Is the content of the caption redundant or meaningless?\n",
    "\n",
    "* For the errors GIT made in terms of the generated captions, try to group them and find reasons for each of the error classes. One class could be, e.g., ''incorrect object name'' (i.e., object not identified), or ''wrong action mentioned'' (if at all).\n",
    "\n",
    "**Remark:** *You can use the function `sample_imgnames()`below to first sample `num_images` image names, and then load the using load_image(). This way, you can keep track of the images you inspected, and use them also as input for another model (see below, VATEX).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3cf3c-1445-4456-82ed-3732f9bc319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_imgnames(imgdir=\"data/IRFL/images/\", num_imgs=10):\n",
    "    return random.sample(os.listdir(imgdir), num_imgs)\n",
    "    \n",
    "img_names = sample_imgnames(num_imgs=3) # sample 3 image names\n",
    "img, _ = load_image(imgname=img_names[0]) # load the first of the 3 image names (index 0)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553aa25-9a86-421b-86ca-88c4d2a71483",
   "metadata": {},
   "source": [
    "You can also try another model: The code below loads the model that was trained on VATEX, a video captioning dataset. \n",
    "* How do the two models compare in terms of the evaluation criteria? Is one model, e.g., more faithful than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1e4bf-424a-4c2e-bed1-2e3739f975ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "\n",
    "# run on the GPU if you have one\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63de35-3153-4bc9-adb5-8bb736e8a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values.to(device)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "print(\"Image name: \", imgname)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb732518-de29-4f79-9a7b-985fdc077e15",
   "metadata": {},
   "source": [
    "### Visual Question Answering (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef62d3-c349-402a-b6f8-f324e9bde1d7",
   "metadata": {},
   "source": [
    "To load the model fine-tuned on [TextVQA](https://huggingface.co/datasets/facebook/textvqa), run the first block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85daad-cd14-4b4d-a57a-58722bf7d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textvqa\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textvqa\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da20aa-d6e4-4bff-8692-1d2411bc4ddb",
   "metadata": {},
   "source": [
    "To load the model fine-tuned on [VQAv2](https://huggingface.co/datasets/HuggingFaceM4/VQAv2) instead, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb610dd4-0436-46cd-a47c-01c692b6c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vqav2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vqav2\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b44d1-6faa-4dd6-a248-b63e5751ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, imgname = load_image(\"data\", \"jackfruit.jpg\")\n",
    "#image, imgname = load_image() # uncomment to load random images from IRFL\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f581db-b2ab-4185-af6f-4752714af987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08897b25-4ead-46aa-a40a-ca75e7f96cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"what colour is the mirror?\" # is the bed white / black / pink\n",
    "question = \"what is shown in the image?\" # is the mirror white?\n",
    "input_ids = processor(text=question, add_special_tokens=False).input_ids\n",
    "input_ids = [processor.tokenizer.cls_token_id] + input_ids\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n",
    "\n",
    "print(\"Image name: \", imgname)\n",
    "print(\"Generated answer:\", processor.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c85fe-91b4-49d7-abdf-b08136a1b5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8350090-df5a-4e1d-ba54-87e75451828e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Sample ten images, and for each image, ask the model three questions. You could ask for *attributes* (e.g., colour, shape, activity), *objects*, or relations (e.g., actions, spatial relations between objects).\n",
    "2. Inspect the answers, and analyse then according to the following criteria: \n",
    "  * **Accuracy:** Is the answer correct?\n",
    "    * Q: What is the proportion of correct answers overall? Calculate that by dividing the correct answers by the total number of questions, i.e., 30.\n",
    "  * **Consistency:**\n",
    "    Does the model respond consistently across different questions? <br/>\n",
    "    * You can test this, e.g., by paraphrasing a question, asking a reconfirming question, or negating the proposition. <br/>\n",
    "      For example: Q: *what is the colour of the apple?* A: `green`\n",
    "      * Reconfirming question: *is the apple green?* A: `yes` is consistent, A: `no` is inconsistent\n",
    "      * Paraphrase: *what is the colour of the fruit?* (provided there is only one fruit shown in the image)\n",
    "      * Negation of *is the apple green?* should yield the opposite answer to *is the apple not green?* (e.g., yes -> no).\n",
    "    * Test for consistency with each of the three questions per image.\n",
    "    * Q: For how many questions is the model inconsistent?\n",
    "  * **Validity:**\n",
    "    Is the answer in the scope of the question? <br/>\n",
    "    For example, a number when asking a counting question, a colour when asking for a colour.\n",
    "  * **Plausibility:**\n",
    "    Is the answer reasonable or does it make sense given the question?<br/>\n",
    "    For example, `red` is a plausible answer to a question about the colour of an apple, but it is implausible for the colour or a lion.\n",
    "\n",
    "2. Inspect the errors the model made and try to find error classes. Possible error classes could be: mistake in visual *grounding*, *relation* between objects / entities not understood, image seems to having been disregarded (*hallucinated* answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c699a6-529a-4b09-b472-242230a84b92",
   "metadata": {},
   "source": [
    "### Further information\n",
    "* [Measuring Faithful and Plausible Visual Grounding in VQA](https://aclanthology.org/2023.findings-emnlp.206.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
